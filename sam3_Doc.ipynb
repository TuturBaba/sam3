{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf79144-216c-4fb0-9610-f3fa1b5d741c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# GitHub\n",
    "git config --global credential.helper store\n",
    "\n",
    "\n",
    "git add sam3_Doc.ipynb sam3_test.ipynb  \n",
    "git commit -m \"Update SAM3 docs and tests\"  \n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26bb5d6-70e1-4fea-9780-b77a4c3b4b16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Concept Segmentation Examples\n",
    "## Segment with Text Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad466b-ca42-4d05-8560-eb44bf345d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.sam import SAM3SemanticPredictor\n",
    "\n",
    "# Initialize predictor with configuration\n",
    "overrides = dict(\n",
    "    conf=0.25,\n",
    "    task=\"segment\",\n",
    "    mode=\"predict\",\n",
    "    model=\"sam3.pt\",\n",
    "    half=True,  # Use FP16 for faster inference\n",
    "    save=True,\n",
    ")\n",
    "predictor = SAM3SemanticPredictor(overrides=overrides)\n",
    "\n",
    "# Set image once for multiple queries\n",
    "predictor.set_image(\"path/to/image.jpg\")\n",
    "\n",
    "# Query with multiple text prompts\n",
    "results = predictor(text=[\"person\", \"bus\", \"glasses\"])\n",
    "\n",
    "# Works with descriptive phrases\n",
    "results = predictor(text=[\"person with red cloth\", \"person with blue cloth\"])\n",
    "\n",
    "# Query with a single concept\n",
    "results = predictor(text=[\"a person\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4c1a4-c74c-49ce-84c4-3510f0492ef6",
   "metadata": {},
   "source": [
    "## Segment with Image Exemplars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5df898-72dd-403b-8783-463bba935eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.sam import SAM3SemanticPredictor\n",
    "\n",
    "# Initialize predictor\n",
    "overrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", model=\"sam3.pt\", half=True, save=True)\n",
    "predictor = SAM3SemanticPredictor(overrides=overrides)\n",
    "\n",
    "# Set image\n",
    "predictor.set_image(\"path/to/image.jpg\")\n",
    "\n",
    "# Provide bounding box examples to segment similar objects\n",
    "results = predictor(bboxes=[[480.0, 290.0, 590.0, 650.0]])\n",
    "\n",
    "# Multiple bounding boxes for different concepts\n",
    "results = predictor(bboxes=[[539, 599, 589, 639], [343, 267, 499, 662]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec5594-ab42-4a0e-9742-3834ebfaab8e",
   "metadata": {},
   "source": [
    "## Feature-based Inference for Efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54ac1c-0880-4b67-8c3b-11983f0a7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics.models.sam import SAM3SemanticPredictor\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "# Initialize predictors\n",
    "overrides = dict(conf=0.50, task=\"segment\", mode=\"predict\", model=\"sam3.pt\", verbose=False)\n",
    "predictor = SAM3SemanticPredictor(overrides=overrides)\n",
    "predictor2 = SAM3SemanticPredictor(overrides=overrides)\n",
    "\n",
    "# Extract features from the first predictor\n",
    "source = \"path/to/image.jpg\"\n",
    "predictor.set_image(source)\n",
    "src_shape = cv2.imread(source).shape[:2]\n",
    "\n",
    "# Setup second predictor and reuse features\n",
    "predictor2.setup_model()\n",
    "\n",
    "# Perform inference using shared features with text prompt\n",
    "masks, boxes = predictor2.inference_features(predictor.features, src_shape=src_shape, text=[\"person\"])\n",
    "\n",
    "# Perform inference using shared features with bounding box prompt\n",
    "masks, boxes = predictor2.inference_features(predictor.features, src_shape=src_shape, bboxes=[[439, 437, 524, 709]])\n",
    "\n",
    "# Visualize results\n",
    "if masks is not None:\n",
    "    masks, boxes = masks.cpu().numpy(), boxes.cpu().numpy()\n",
    "    im = cv2.imread(source)\n",
    "    annotator = Annotator(im, pil=False)\n",
    "    annotator.masks(masks, [colors(x, True) for x in range(len(masks))])\n",
    "\n",
    "    cv2.imshow(\"result\", annotator.result())\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e9cec-00d5-4f10-9185-11d027fdbd49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Video Concept Segmentation\n",
    "## Track Concepts Across Video with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3dff8-a8b5-4917-a4b5-05988acfe70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.sam import SAM3VideoPredictor\n",
    "\n",
    "# Create video predictor\n",
    "overrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", model=\"sam3.pt\", half=True)\n",
    "predictor = SAM3VideoPredictor(overrides=overrides)\n",
    "\n",
    "# Track objects using bounding box prompts\n",
    "results = predictor(source=\"path/to/video.mp4\", bboxes=[[706.5, 442.5, 905.25, 555], [598, 635, 725, 750]], stream=True)\n",
    "\n",
    "# Process and display results\n",
    "for r in results:\n",
    "    r.show()  # Display frame with segmentation masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c34884-617b-443f-be3c-84d83d2032e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T14:07:18.489673Z",
     "iopub.status.busy": "2026-01-15T14:07:18.489241Z",
     "iopub.status.idle": "2026-01-15T14:07:18.494981Z",
     "shell.execute_reply": "2026-01-15T14:07:18.493750Z",
     "shell.execute_reply.started": "2026-01-15T14:07:18.489632Z"
    },
    "tags": []
   },
   "source": [
    "## Track Concepts with Text Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c136d03-7f08-408b-9bbe-e5f189f32905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.sam import SAM3VideoSemanticPredictor\n",
    "\n",
    "# Initialize semantic video predictor\n",
    "overrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", imgsz=640, model=\"sam3.pt\", half=True, save=True)\n",
    "predictor = SAM3VideoSemanticPredictor(overrides=overrides)\n",
    "\n",
    "# Track concepts using text prompts\n",
    "results = predictor(source=\"path/to/video.mp4\", text=[\"person\", \"bicycle\"], stream=True)\n",
    "\n",
    "# Process results\n",
    "for r in results:\n",
    "    r.show()  # Display frame with tracked objects\n",
    "\n",
    "# Alternative: Track with bounding box prompts\n",
    "results = predictor(\n",
    "    source=\"path/to/video.mp4\",\n",
    "    bboxes=[[864, 383, 975, 620], [705, 229, 782, 402]],\n",
    "    labels=[1, 1],  # Positive labels\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063be4a-0f0b-4fa3-8612-be20d9d53494",
   "metadata": {},
   "source": [
    "# Visual Prompts (SAM 2 Compatibility)\n",
    "## SAM 3 maintains full backward compatibility with SAM 2's visual prompting for single-object segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e9510-86a5-493f-8dc1-f1159f375ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "\n",
    "model = SAM(\"sam3.pt\")\n",
    "\n",
    "# Single point prompt - segments object at specific location\n",
    "results = model.predict(source=\"path/to/image.jpg\", points=[900, 370], labels=[1])\n",
    "results[0].show()\n",
    "\n",
    "# Multiple points - segments single object with multiple point hints\n",
    "results = model.predict(source=\"path/to/image.jpg\", points=[[400, 370], [900, 370]], labels=[1, 1])\n",
    "\n",
    "# Box prompt - segments object within bounding box\n",
    "results = model.predict(source=\"path/to/image.jpg\", bboxes=[100, 150, 300, 400])\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df66833-076b-4114-af15-44608744d744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eca085-5a3c-461f-acb2-9b0d109acadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cdf94-e4ca-4d36-8e06-4a23c1669fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c34261-945f-4ce7-98fb-365a8324d19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e73b7-c8a4-4a52-8a51-36fbc700b487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24743c3f-12f3-4bce-b380-a3056e35e5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3af7a3-7611-4b41-82cf-465f72ae0398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321afef7-0825-42da-a70f-2e05cf421573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171ad86-b940-4d3b-8ee8-e493247ffc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58db87a-23ba-45e2-927c-59550971eb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe8534-9d0e-4c3f-994c-5aa4ed67031f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdc5e4-c2ce-4706-a9e8-e5b18e0375f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b705ed-3aab-40d9-989e-214441f014dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417342a-c585-446b-ab27-2f1a6bdee5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
